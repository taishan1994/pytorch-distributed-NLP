# pytorch-distributed-NLP
pytorchå•æœºå¤šå¡åˆ†å¸ƒå¼è®­ç»ƒ-ä¸­æ–‡æ–‡æœ¬åˆ†ç±»ã€‚ä¸€ç›´æƒ³å°è¯•æ¥ç€ï¼Œè‹¦äºæ²¡æœ‰å¡ï¼Œåªå¥½èŠ±è¿‡å¹´çš„å‹å²é’±å»Autodlä¸Šç§Ÿäº†ä¸¤å¼ å¡ã€‚

# ç¯å¢ƒ

```Linux+torch==2.0+transformers==4.28.1```

# å¯¹æ¯”

| æ–¹æ³•                         | è€—æ—¶(åˆ†é’Ÿ)             |
| ---------------------------- | ---------------------- |
| å•GPU                        | 2.8276                 |
| dataparallel                 | 2.0301                 |
| distributed                  | 1.4120                 |
| distributed-multiprocess     | 1.4921                 |
| distributed-multiprocess-amp | 0.6336                 |
| horovod                      | 5.1228ï¼ˆå­˜åœ¨ä¸€äº›é—®é¢˜ï¼‰ |
| deepspeed                    | 1.0114                 |
| accelerate                   | 1.3667                 |

# å•GPUè®­ç»ƒ

è¿è¡Œï¼š```python single-gpu-cls.py```

![image-20230506162110739](README.assets/image-20230506162110739.png)

```python
ã€trainã€‘ epochï¼š1/1 stepï¼š1/288 lossï¼š1.817216
ã€trainã€‘ epochï¼š1/1 stepï¼š2/288 lossï¼š1.850495
ã€trainã€‘ epochï¼š1/1 stepï¼š3/288 lossï¼š1.626242
ã€trainã€‘ epochï¼š1/1 stepï¼š4/288 lossï¼š1.641953
ã€trainã€‘ epochï¼š1/1 stepï¼š5/288 lossï¼š1.678075
```

å•GPUæ²¡ä»€ä¹ˆå¥½è¯´çš„ã€‚éœ€è¦ä¸€æçš„æ˜¯ï¼Œé€šå¸¸ä½¿ç”¨```device = torch.device("cuda" if torch.cuda.is_available else "cpu")```å°†æ¨¡å‹å’Œæ•°æ®æ”¾åœ¨æŒ‡å®šçš„GPUæˆ–è€…CPUä¸Šã€‚

# DataParallelåˆ†å¸ƒå¼è®­ç»ƒ

è¿è¡Œï¼š```python multi-gpu-dataparrel-cls.py```

ä¸€èˆ¬æµç¨‹ï¼š

```python
gpu_ids = [0,1]
# ç¬¬ä¸€æ­¥ï¼šå®šä¹‰æ¨¡å‹
model = BertForSequenceClassification.from_pretrained(args.model_path, config=config)
# ç¬¬äºŒæ­¥ï¼šè¿™é‡Œæˆ‘ä»¬ä¸åœ¨ä½¿ç”¨xxx.to(device)è¿™ç§æ–¹å¼äº†ï¼Œç›´æ¥ç”¨xxx.cuda()å°†æ¨¡å‹æˆ–è€…æ•°æ®æ”¾åœ¨GPUä¸Šã€‚
model.cuda()
# ç¬¬ä¸‰æ­¥ï¼šnn.DataParallelå°è£…æ¨¡å‹ï¼Œå¹¶æŒ‡å®šgpu_idsçš„ç¼–ç ï¼Œæ¯”å¦‚[0,1]ä½¿ç”¨ç¬¬1å—å’Œç¬¬2å—GPU,output_deviceæ˜¯åœ¨é‚£ä¸€å—GPUä¸Šè¿›è¡Œæ±‡æ€»è®¡ç®—
model = nn.DataParallel(model, device_ids=args.gpu_ids, output_device=args.gpu_ids[0])
```

![image-20230506162006099](README.assets/image-20230506162006099.png)

```pythn
ã€trainã€‘ epochï¼š1/1 stepï¼š1/288 lossï¼š1.848520
ã€trainã€‘ epochï¼š1/1 stepï¼š2/288 lossï¼š1.912011
ã€trainã€‘ epochï¼š1/1 stepï¼š3/288 lossï¼š1.571108
ã€trainã€‘ epochï¼š1/1 stepï¼š4/288 lossï¼š1.668026
ã€trainã€‘ epochï¼š1/1 stepï¼š5/288 lossï¼š1.558577
```

æ€»çš„stepæ•°å¹¶æ²¡æœ‰å‡å°‘ï¼Œä½†æ˜¯æ€»å ç”¨çš„GPUæ˜¾å­˜åˆ†å¸ƒåœ¨ä¸¤å¼ æ˜¾å¡ä¸Šã€‚

ä¼˜ç‚¹ï¼š

- æ˜“äºä½¿ç”¨ã€‚

ç¼ºç‚¹ï¼š

- å®ƒä½¿ç”¨ä¸€ä¸ªè¿›ç¨‹æ¥è®¡ç®—æ¨¡å‹æƒé‡ï¼Œç„¶ååœ¨æ¯ä¸ªæ‰¹å¤„ç†æœŸé—´å°†åˆ†å‘åˆ°æ¯ä¸ªGPUï¼Œå› æ­¤é€šä¿¡å¾ˆå¿«æˆä¸ºä¸€ä¸ªç“¶é¢ˆï¼ŒGPUåˆ©ç”¨ç‡é€šå¸¸å¾ˆä½ã€‚
- è¦æ±‚æ‰€æœ‰çš„GPUéƒ½åœ¨åŒä¸€ä¸ªèŠ‚ç‚¹ä¸Šï¼ˆä¸æ”¯æŒåˆ†å¸ƒå¼ï¼‰ã€‚
- ä¸èƒ½ä½¿ç”¨[Apex](https://link.zhihu.com/?target=https%3A//nvidia.github.io/apex/amp.html)è¿›è¡Œ[æ··åˆç²¾åº¦è®­ç»ƒ](https://link.zhihu.com/?target=https%3A//devblogs.nvidia.com/mixed-precision-training-deep-neural-networks/)ã€‚

# Distributedåˆ†å¸ƒå¼è®­ç»ƒ

è¿è¡Œï¼š

```python
python -m torch.distributed.launch --nnode=1 --node_rank=0 --nproc_per_node=2 --use_env multi-gpu-distributed-cls.py --local_world_size=2

æˆ–è€…

python -m torch.distributed.launch --nnode=1 --node_rank=0 --nproc_per_node=2 multi-gpu-distributed-cls.py --local_world_size=2
```

![image-20230506165535656](README.assets/image-20230506165535656.png)

```python
[5574] Initializing process group with: {'MASTER_ADDR': '127.0.0.1', 'MASTER_PORT': '29500', 'RANK': '0', 'WORLD_SIZE': '2', 'LOCAL_RANK': '0'}
[5575] Initializing process group with: {'MASTER_ADDR': '127.0.0.1', 'MASTER_PORT': '29500', 'RANK': '1', 'WORLD_SIZE': '2', 'LOCAL_RANK': '1'}
[5575] rank = 1, world_size = 2, n = 1, device_ids = [1] 
[5574] rank = 0, world_size = 2, n = 1, device_ids = [0] 

ã€trainã€‘ epochï¼š1/1 stepï¼š1/144 lossï¼š1.765123
ã€trainã€‘ epochï¼š1/1 stepï¼š2/144 lossï¼š1.646639
ã€trainã€‘ epochï¼š1/1 stepï¼š3/144 lossï¼š1.780050
ã€trainã€‘ epochï¼š1/1 stepï¼š4/144 lossï¼š1.642378
ã€trainã€‘ epochï¼š1/1 stepï¼š5/144 lossï¼š1.599494
```

è¯´æ˜ï¼š

- nnodeï¼š1ä¸ªèŠ‚ç‚¹

- node_rankï¼šèŠ‚ç‚¹æ ‡è¯†

- nproc_per_nodeï¼šæ¯ä¸ªèŠ‚ç‚¹2ä¸ªè¿›ç¨‹(GPUæ•°ç›®)

- use_envï¼šä½¿ç”¨ç³»ç»Ÿçš„ç¯å¢ƒå˜é‡

- local_world_sizeï¼šè‡ªå®šä¹‰çš„ï¼ŒGPUçš„æ•°é‡

æ¨¡å‹ä¼šåˆå§‹åŒ–ä¸¤æ¬¡ï¼Œå› ä¸ºèµ·äº†ä¸¤ä¸ªè¿›ç¨‹ï¼Œæ¯ä¸ªè¿›ç¨‹éƒ½éœ€è¦åˆå§‹åŒ–æ¨¡å‹ä¸€æ¬¡ã€‚è¿™é‡Œè¯´ä¸€ä¸‹rankå’Œlocal_rankã€‚åœ¨å•æœºå¤šå¡çš„æƒ…å†µä¸‹rankå’Œlocal_rankæ˜¯æ²¡æœ‰åŒºåˆ«çš„ï¼Œå®ƒæ ‡è¯†äº†å½“å‰ä½¿ç”¨çš„ç¬¬å‡ å—GPUã€‚

æ€»çš„stepæ•°å‡å°‘äº†ä¸€åŠï¼Œå› ä¸ºå¹¶è¡Œå¤„ç†æ•°æ®ã€‚

åœ¨è®­ç»ƒçš„æ—¶å€™è¿›è¡ŒéªŒè¯çš„æ—¶å€™ï¼Œéœ€è¦æŠŠæ‰€æœ‰GPUä¸Šçš„ç»“æœè¿›è¡Œreduceï¼Œå†è¿›è¡Œè®¡ç®—ï¼š

```python
def output_reduce(self, outputs, targets):
    output_gather_list = [torch.zeros_like(outputs) for _ in range(self.args.local_world_size)]
    # æŠŠæ¯ä¸€ä¸ªGPUçš„è¾“å‡ºèšåˆèµ·æ¥
    dist.all_gather(output_gather_list, outputs)

    outputs = torch.cat(output_gather_list, dim=0)
    target_gather_list = [torch.zeros_like(targets) for _ in range(self.args.local_world_size)]
    # æŠŠæ¯ä¸€ä¸ªGPUçš„è¾“å‡ºèšåˆèµ·æ¥
    dist.all_gather(target_gather_list, targets)
    targets = torch.cat(target_gather_list, dim=0)
    return outputs, targets
```

åœ¨åŠ è½½ä¿å­˜å¥½çš„æ¨¡å‹è¿›è¡Œæµ‹è¯•çš„æ—¶å€™ï¼Œè¦å…ˆå°†æ¨¡å‹è¿›è¡Œå°è£…å†åŠ è½½ä¿å­˜å¥½çš„æƒé‡ï¼š

```python
model = BertForSequenceClassification.from_pretrained(args.model_path, config=config)
model.cuda()
model = torch.nn.parallel.DistributedDataParallel(model, device_ids=args.device_ids)
model.load_state_dict(torch.load(args.ckpt_path))
```

ä¸€èˆ¬æµç¨‹ï¼š

```python
# ç¬¬é›¶æ­¥ï¼šéœ€è¦å®šä¹‰ä¸€ä¸ªå‚æ•°
import argparse
parser = argparse.ArgumentParser()
# ========================================
# è¿™ä¸ªå¿…é¡»è¢«é¢„å…ˆå®šä¹‰
parser.add_argument("--local-rank", type=int, default=0)
# ç¬¬ä¸€æ­¥ï¼šåˆå§‹åŒ–
dist.init_process_group(backend="nccl")
# ========================================
# ç¬¬äºŒæ­¥ï¼šDistributedSamplerï¼Œéœ€è¦æ³¨æ„çš„æ˜¯åœ¨train_loaderé‡Œé¢ä¸èƒ½å†è®¾ç½®shuffle=True
train_dataset = ClsDataset(train_data)
train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)
train_loader = DataLoader(train_dataset,
                              batch_size=args.train_batch_size,
                              num_workers=2,
                              collate_fn=collate.collate_fn,
                              sampler=train_sampler)
total_step = len(train_loader) * args.epochs
args.total_step = total_step
dev_dataset = ClsDataset(dev_data)
dev_sampler = torch.utils.data.distributed.DistributedSampler(dev_dataset)
dev_loader = DataLoader(dev_dataset,
                            batch_size=args.dev_batch_size,
                            shuffle=False,
                            num_workers=2,
                            collate_fn=collate.collate_fn,
                            sampler=dev_sampler)
# ========================================
# ç¬¬ä¸‰æ­¥ï¼šå°è£…æ¨¡å‹
self.model = BertForSequenceClassification.from_pretrained(args.model_path,
                                                                   config=self.config)
self.model.cuda()
self.model = torch.nn.parallel.DistributedDataParallel(self.model, device_ids=args.device_ids)
# ========================================
for epoch in range(1, self.args.epochs + 1):
    # ç¬¬å››æ­¥ï¼šè®­ç»ƒæ—¶æ¯ä¸€ä¸ªepochæ‰“ä¹±æ•°æ®
    train_sampler.set_epoch(epoch)
    for step, batch_data in enumerate(train_loader):
        self.model.train()
        logits, label = self.on_step(batch_data)
        """
        def on_step(self, batch_data):
            # ç¬¬äº”æ­¥ï¼šæ ¹æ®local_rankå°†æ•°æ®åˆ†å‘ç»™æŒ‡å®šçš„GPU
            label = batch_data["label"].cuda()
            input_ids = batch_data["input_ids"].cuda()
            token_type_ids = batch_data["token_type_ids"].cuda()
            attention_mask = batch_data["attention_mask"].cuda()
            output = self.model(input_ids=input_ids,
                                token_type_ids=token_type_ids,
                                attention_mask=attention_mask,
                                labels=label)
            logits = output[1]
            return logits, label
        """
        # ========================================
        loss = self.criterion(logits, label)
        # ç¬¬å…­æ­¥ï¼šç­‰å¾…æ‰€æœ‰GPU
        torch.distributed.barrier()
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        # ========================================
        # ç¬¬ä¸ƒæ­¥ï¼šreduceè®¡ç®—æŸå¤±(æ±‡æ€»æ‰€æœ‰GPUä¸Šçš„ç»“æœ)
        loss = self.loss_reduce(loss)
        # ========================================
        # ç¬¬å…«æ­¥ï¼šåœ¨ä¸»rankæ‰“å°æŒ‡æ ‡
        if self.args.local_rank == 0:
            print("ã€trainã€‘ epochï¼š{}/{} stepï¼š{}/{} lossï¼š{:.6f}".format(
                epoch, self.args.epochs, gloabl_step, self.args.total_step, loss
            ))
        # ========================================
        # ç¬¬ä¹æ­¥ï¼šåœ¨ä¸»rankä¿å­˜æ¨¡å‹
        if gloabl_step % self.args.eval_step == 0:
            loss, accuracy = self.dev(dev_loader)
            if self.args.local_rank == 0:
                print("ã€devã€‘ lossï¼š{:.6f} accuracyï¼š{:.4f}".format(loss, accuracy))
                if accuracy > best_acc:
                    best_acc = accuracy
                    print("ã€best accuracyã€‘ {:.4f}".format(best_acc))
                    torch.save(self.model.state_dict(), self.args.ckpt_path)
# ========================================
# æœ€åä¸€æ­¥
dist.destroy_process_group()
```

# distributedåˆ†å¸ƒå¼è®­ç»ƒ-multiprocesså¯åŠ¨

è¿è¡Œï¼š```python multi-gpu-distributed-mp-cls.py --local_world_size=2```

![image-20230508100415861](README.assets/image-20230508100415861.png)

```python
ã€trainã€‘ epochï¼š1/1 stepï¼š1/144 lossï¼š1.765123
ã€trainã€‘ epochï¼š1/1 stepï¼š2/144 lossï¼š1.646639
ã€trainã€‘ epochï¼š1/1 stepï¼š3/144 lossï¼š1.780050
ã€trainã€‘ epochï¼š1/1 stepï¼š4/144 lossï¼š1.642378
ã€trainã€‘ epochï¼š1/1 stepï¼š5/144 lossï¼š1.599494
```

è¯´æ˜ï¼š

ä½¿ç”¨æ—¶ï¼Œåªéœ€è¦è°ƒç”¨ torch.multiprocessing.spawnï¼Œtorch.multiprocessing å°±ä¼šå¸®åŠ©æˆ‘ä»¬è‡ªåŠ¨åˆ›å»ºè¿›ç¨‹ã€‚ä¾‹å¦‚æœ‰ä¸¤å¼ æ˜¾å¡ï¼Œå°±è®¾ç½® nprocs=2å¯åŠ¨ä¸¤ä¸ªè¿›ç¨‹ã€‚

```python
mp.spawn(main_worker, nprocs=2, args=(args,))
```

ä¸»å‡½æ•°main_workeré‡Œé¢çš„ç¬¬ä¸€ä¸ªå‚æ•°å¿…é¡»æ˜¯local_rankï¼Œä¼šè‡ªåŠ¨ç»™å®ƒèµ‹å€¼ã€‚ç„¶åæˆ‘ä»¬éœ€è¦ä¿®æ”¹ï¼š

```python
dist.init_process_group(backend="nccl", init_method="tcp://localhost:12345", world_size=local_world_size, rank=local_rank)
```

ç”±äºç¯å¢ƒå˜é‡é‡Œé¢æ²¡æœ‰æˆ‘ä»¬æ‰€éœ€è¦çš„å‚æ•°äº†ï¼Œæˆ‘ä»¬éœ€è¦è‡ªå·±å®šä¹‰å¹¶ä¼ å…¥åˆ°init_process_groupé‡Œé¢ã€‚

ä¸€èˆ¬æµç¨‹ï¼š

```python
main_worker(local_rank, args):
    # é™¤äº†è¿™é‡Œè¦å¡«å…¥å‚æ•°å¤–ï¼Œå…¶ä½™å’Œdistributedå¯åŠ¨åŸºæœ¬ä¸€è‡´
    dist.init_process_group(backend="nccl", init_method="tcp://localhost:12345", world_size=local_world_size, rank=local_rank)
if __name__ == "__main__":
    mp.spawn(main_worker, nprocs=2, args=(args,))
```

# AMPæ··åˆç²¾åº¦è®­ç»ƒ

è¿è¡Œï¼š```python multi-gpu-distributed-mp-amp-cls.py --local_world_size=2```

ä»1.6ç‰ˆæœ¬å¼€å§‹ï¼ŒPytorchåŸç”Ÿæ”¯æŒè‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒï¼Œå¹¶å·²è¿›å…¥ç¨³å®šé˜¶æ®µï¼Œ

![image-20230508154206416](README.assets/image-20230508154206416.png)

```python
ã€trainã€‘ epochï¼š1/1 stepï¼š1/144 lossï¼š1.799011
ã€trainã€‘ epochï¼š1/1 stepï¼š2/144 lossï¼š1.654877
ã€trainã€‘ epochï¼š1/1 stepï¼š3/144 lossï¼š1.808228
ã€trainã€‘ epochï¼š1/1 stepï¼š4/144 lossï¼š1.615723
ã€trainã€‘ epochï¼š1/1 stepï¼š5/144 lossï¼š1.652313
```

ä¸€èˆ¬æµç¨‹ï¼š

åœ¨distributedçš„åŸºç¡€ä¸Šï¼Œé¢å¤–æ·»åŠ ä»¥ä¸‹ä»£ç å³å¯ï¼š

```python
if self.args.use_amp:
    scaler = torch.cuda.amp.GradScaler()
for epoch in range(1, self.args.epochs + 1):
    train_sampler.set_epoch(epoch)
    for step, batch_data in enumerate(train_loader):
        self.model.train()
        if self.args.use_amp:
            with torch.cuda.amp.autocast():
                logits, label = self.on_step(batch_data)
                loss = self.criterion(logits, label)
                torch.distributed.barrier()
                scaler.scale(loss).backward()
                scaler.step(self.optimizer)
                scaler.update()
        else:
            logits, label = self.on_step(batch_data)
            loss = self.criterion(logits, label)
            torch.distributed.barrier()
            loss.backward()
            self.optimizer.step()
```

è®­ç»ƒçš„æ—¶é•¿æ˜æ˜¾å˜çŸ­äº†ï¼Œæ¨¡å‹çš„æ€§èƒ½ä¹Ÿæ²¡æœ‰ä¸‹é™ï¼Œéå¸¸ä¸é”™ã€‚

# horovodåˆ†å¸ƒå¼è®­ç»ƒ

ä¾èµ–ï¼š```horovod==0.27.0```

è¿è¡Œï¼š```horovodrun -np 2 -H localhost:2 python multi-gpu-horovod-cls.py```

![image-20230508142608957](README.assets/image-20230508142608957.png)

```python
[0]<stdout>:ã€trainã€‘ epochï¼š1/1 stepï¼š1/144 lossï¼š1.798987
[0]<stdout>:ã€trainã€‘ epochï¼š1/1 stepï¼š2/144 lossï¼š1.654544
[0]<stdout>:ã€trainã€‘ epochï¼š1/1 stepï¼š3/144 lossï¼š1.808229
[0]<stdout>:ã€trainã€‘ epochï¼š1/1 stepï¼š4/144 lossï¼š1.616281
[0]<stdout>:ã€trainã€‘ epochï¼š1/1 stepï¼š5/144 lossï¼š1.652950
```

ä¸€èˆ¬æµç¨‹ï¼š

```python
hvd.init()
args.local_rank = hvd.local_rank()

torch.cuda.set_device(args.local_rank)

collate = Collate(tokenizer, args.max_seq_len)
train_dataset = ClsDataset(train_data)
train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset, num_replicas=hvd.size(), rank=hvd.rank())
train_loader = DataLoader(train_dataset,
                          batch_size=args.train_batch_size,
                          num_workers=2,
                          collate_fn=collate.collate_fn,
                          sampler=train_sampler)
total_step = len(train_loader) * args.epochs
args.total_step = total_step
dev_dataset = ClsDataset(dev_data)
dev_sampler = torch.utils.data.distributed.DistributedSampler(dev_dataset, num_replicas=hvd.size(), rank=hvd.rank())
dev_loader = DataLoader(dev_dataset,
                        batch_size=args.dev_batch_size,
                        shuffle=False,
                        num_workers=2,
                        collate_fn=collate.collate_fn,
                        sampler=dev_sampler)

# è¿™é‡Œéœ€è¦æ³¨æ„ï¼Œä¸éœ€è¦å†å°è£…äº†
model = BertForSequenceClassification.from_pretrained(args.model_path,
                                                          config=config)
model.cuda()
hvd.broadcast_parameters(model.state_dict(), root_rank=0)
optimizer = build_optimizer(model, args)

compression = hvd.Compression.fp16
optimizer = hvd.DistributedOptimizer(
    optimizer,
    named_parameters=model.named_parameters(),
    compression=compression)
hvd.broadcast_optimizer_state(optimizer, root_rank=0)
```

å…¶ä½™çš„å’Œpytorchè‡ªå¸¦çš„distributedå·®ä¸å¤šï¼Œè®¡ç®—losså’Œouptputçš„æ—¶å€™éœ€è¦æ³¨æ„å…¶å®šä¹‰çš„æ–¹æ³•çš„åŒºåˆ«ï¼Œå…·ä½“å¯å‚è€ƒå…¶æ–‡æ¡£ã€‚

æ•´ä½“æµç¨‹æ²¡æœ‰é—®é¢˜ï¼Œä½†å­˜åœ¨ä¸€äº›é—®é¢˜ï¼š

- è®­ç»ƒæ—¶é•¿è¾ƒé•¿ã€‚
- æ¨¡å‹å¹¶æ²¡æœ‰è¢«æœ‰æ•ˆçš„è®­ç»ƒã€‚

# deepspeedåˆ†å¸ƒå¼è®­ç»ƒ

```python
pip install deepspeed
sudo apt-get update
sudo apt-get install openmpi-bin libopenmpi-dev
pip install mpi4py
```

è¿è¡Œï¼š```deepspeed -np 2 -H localhost:2 multi-gpu-deepspeed-cls.py```

![image-20230508171612641](README.assets/image-20230508171612641.png)

```python
ã€trainã€‘ epochï¼š1/1 stepï¼š1/288 lossï¼š1.817383
ã€trainã€‘ epochï¼š1/1 stepï¼š2/288 lossï¼š1.851562
ã€trainã€‘ epochï¼š1/1 stepï¼š3/288 lossï¼š1.679688
ã€trainã€‘ epochï¼š1/1 stepï¼š4/288 lossï¼š1.725586
ã€trainã€‘ epochï¼š1/1 stepï¼š5/288 lossï¼š1.826172
```

å¦‚æœæŠ¥é”™ï¼š

- ModuleNotFoundError: No module named 'torch._sixï¼šæ‰¾åˆ°æŠ¥é”™çš„æ–‡ä»¶ï¼Œ

```python
æ³¨é‡Šæ‰ï¼šfrom torch._six import string_classes
åŠ å…¥ï¼š
int_classes = int
string_classes = str
å¦‚æœè¿˜æŠ¥é”™ï¼šNameError: name 'inf' is not defined
æ‰¾åˆ°æ–‡ä»¶ä¸­çš„é‚£ä¸€è¡Œï¼Œ
å‰é¢åŠ å…¥ï¼š
import math
inf = math.inf
```

ä¸€èˆ¬è¿‡ç¨‹ï¼š

```python
import torch
import deepspeed

# åˆå§‹åŒ–DeepSpeedå¼•æ“
config = {
    "train_micro_batch_size_per_gpu": 32,
    "optimizer": {
        "type": "Adam",
        "params": {
            "lr": 1e-4
        }
    }
}

model.cuda()
model_engine, optimizer, _, _ = deepspeed.initialize(config_params=configï¼Œ
                                              model=model,
                                              model_parameters=model.parameters())

# è·å–æœ¬åœ°rankå’Œè®¾å¤‡
local_rank = engine.local_rank
device = engine.device

# åŠ è½½æ•°æ®
train_loader = ...
dev_loader = ...

# è®­ç»ƒå¾ªç¯
for epoch in range(10):
    total_loss = 0.0
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.cuda(), target.cuda()
        # å‰å‘ä¼ æ’­å’ŒæŸå¤±è®¡ç®—
        logits = model_engine.forward(data)
        loss = CrossEntropyLoss(data, target)
        # åå‘ä¼ æ’­å’Œä¼˜åŒ–å™¨æ›´æ–°
        model_engine.backward(loss)
        model_engine.step()
        ...
```

å…¶ä½™çš„lossçš„reductçš„outputçš„allgatheréƒ½å¯ä»¥ä½¿ç”¨pytorchåŸç”Ÿçš„ã€‚

éœ€è¦æ³¨æ„çš„æ˜¯åœ¨ZeROç¬¬3é˜¶æ®µï¼Œæ¨¡å‹è¢«åˆ’åˆ†åˆ°ä¸åŒçš„GPUäº†ï¼Œè¦æ³¨æ„ä¿å­˜æ¨¡å‹çš„æ–¹å¼ã€‚

æµ‹è¯•çš„æ—¶å€™å‘ç°æ¯å—GPUå¯¹æ¯æ‰¹æ•°æ®éƒ½è¿›è¡Œè®¡ç®—äº†ä¸€æ¬¡ï¼Œè¿™é‡Œå¯èƒ½éœ€è¦åšäº›ä¿®æ”¹ï¼Œæš‚æ—¶è¿˜æ²¡æ‰¾åˆ°ç›¸å…³çš„æ–¹æ³•ã€‚

```python
              precision    recall  f1-score   support

          å…¶ä»–       0.64      0.66      0.65       546
          å–œå¥½       0.49      0.70      0.57       224
          æ‚²ä¼¤       0.59      0.52      0.55       228
          åŒæ¶       0.38      0.38      0.38       240
          æ„¤æ€’       0.56      0.32      0.41       124
          é«˜å…´       0.72      0.62      0.67       238

    accuracy                           0.57      1600
   macro avg       0.56      0.53      0.54      1600
weighted avg       0.58      0.57      0.57      1600
```

# accelerateåˆ†å¸ƒå¼è®­ç»ƒ

è¿è¡Œï¼š

```python
accelerate launch multi-gpu-accelerate-cls.py

æˆ–è€…

python -m torch.distributed.launch --nproc_per_node 2 --use_env multi-gpu-accelerate-cls.py
```

![image-20230509113417401](C:\Users\Administrator\Desktop\github\pytorch-distributed\README.assets\image-20230509113417401.png)

```python
ã€trainã€‘ epochï¼š1/1 stepï¼š1/144 lossï¼š1.795169
ã€trainã€‘ epochï¼š1/1 stepï¼š2/144 lossï¼š1.744665
ã€trainã€‘ epochï¼š1/1 stepï¼š3/144 lossï¼š1.631625
ã€trainã€‘ epochï¼š1/1 stepï¼š4/144 lossï¼š1.543691
ã€trainã€‘ epochï¼š1/1 stepï¼š5/144 lossï¼š1.788955
```

åŒæ ·çš„ï¼Œåœ¨è¿›è¡Œæµ‹è¯•çš„æ—¶å€™æ¯å—GPUéƒ½è®¡ç®—äº†ä¸€æ¬¡æ•°æ®ã€‚

```python
              precision    recall  f1-score   support

          å…¶ä»–       0.64      0.68      0.66       546
          å–œå¥½       0.48      0.63      0.55       224
          æ‚²ä¼¤       0.62      0.48      0.54       228
          åŒæ¶       0.39      0.53      0.45       240
          æ„¤æ€’       0.57      0.19      0.29       124
          é«˜å…´       0.76      0.56      0.65       238

    accuracy                           0.57      1600
   macro avg       0.58      0.51      0.52      1600
weighted avg       0.59      0.57      0.56      1600
```

ä¸€èˆ¬æµç¨‹ï¼š

```python
train_loader = ...
dev_loader = ...

accelerator = Accelerator()
args.local_rank = int(dist.get_rank())
print(args.local_rank)
model_engine, optimizer_engine, train_loader_engine, dev_loader_engine = accelerator.prepare(
    model, optimizer, train_loader, dev_loader
)
```

å…¶ä½™å’ŒdistributedåŸºæœ¬ä¿æŒä¸€è‡´ã€‚éœ€è¦æ³¨æ„çš„æ˜¯æˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨accelerateè‡ªå¸¦çš„ä¸€äº›apiæ“ä½œï¼Œéœ€è¦æŸ¥çœ‹å…¶æ–‡æ¡£ã€‚

# è¡¥å……

- ä¸éš¾å‘ç°å¤§å¤šæƒ…å†µä¸‹åŸºæœ¬çš„æµç¨‹æ˜¯å·®ä¸å¤šçš„ã€‚
- æˆ‘ä»¬éœ€è¦ä¸€ä¸ªç®¡å®¶å¸®æˆ‘ä»¬ç®¡ç†æ¨¡å‹ã€æ•°æ®ã€å‚æ•°ç­‰ä¿¡æ¯æ€ä¹ˆåˆ†é…åˆ°ä¸åŒçš„GPUä¸Šï¼Œè¿™ä¸ªç®¡å®¶å¯ä»¥æ˜¯åŸç”Ÿdistributedæˆ–è€…æ˜¯accelerateã€deepspeedã€‚
- éœ€è¦æ³¨æ„åˆ°åº•æ˜¯ä¸åŒGPUå¤„ç†ç›¸åŒæ•°æ®è¿˜æ˜¯ä¸åŒæ•°æ®ï¼Œæ¯”å¦‚deepspeedã€accelerateã€‚
- ä»¥ä¸Šä»£ç å¯èƒ½åªæ˜¯ä¸€ä¸ªåŸºæœ¬çš„ä½¿ç”¨ï¼Œæ›´é«˜çº§çš„ä½¿ç”¨å¯èƒ½è¿˜éœ€è¦è‡ªè¡Œå»æŸ¥é˜…ç›¸å…³çš„èµ„æ–™ã€‚

# å‚è€ƒ

> [PyTorchåˆ†å¸ƒå¼è®­ç»ƒç®€æ˜æ•™ç¨‹(2022æ›´æ–°ç‰ˆ) - çŸ¥ä¹ (zhihu.com)](https://zhuanlan.zhihu.com/p/113694038)
>
> [tczhangzhi/pytorch-distributed: A quickstart and benchmark for pytorch distributed training. (github.com)](https://github.com/tczhangzhi/pytorch-distributed)
>
> [Pytorch åˆ†å¸ƒå¼è®­ç»ƒçš„å‘ï¼ˆuse_env, loacl_rank) - çŸ¥ä¹ (zhihu.com)](https://zhuanlan.zhihu.com/p/501632575)
>
> https://pytorch.org/docs/stable/elastic/run.html
>
> https://www.w3cschool.cn/article/76555860.htm
>
> [API â€” Horovod documentation](https://horovod.readthedocs.io/en/stable/api.html?highlight=allreduce#module-horovod.torch)
>
> [ChatGPT - Poe](https://poe.com/ChatGPT)
>
> [Model Checkpointing â€” DeepSpeed 0.9.3 documentation](https://deepspeed.readthedocs.io/en/latest/model-checkpointing.html#deepspeed.DeepSpeedEngine.save_checkpoint)
>
> [20åˆ†é’Ÿåƒæ‰accelerateæ¨¡å‹åŠ é€Ÿå·¥å…·ğŸ˜‹ - çŸ¥ä¹ (zhihu.com)](https://zhuanlan.zhihu.com/p/599274899)
